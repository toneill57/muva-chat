# Conversation Memory E2E Test Report

## Overview

This document summarizes the E2E testing approach for the Conversation Memory Auto-Compression system.

**ğŸ¨ Multi-Tenant Support**: All tests support configurable `tenant_id`. See [MULTI_TENANT_TESTING.md](./MULTI_TENANT_TESTING.md) for details.

## Test Files Created

### 1. `/e2e/conversation-memory.spec.ts`
Comprehensive Playwright E2E test suite with 6 test scenarios:

- **TEST 1**: No compression with 18 messages
- **TEST 2**: First compression at 20 messages
- **TEST 3**: Second compression at 30 messages
- **TEST 4**: Compression content quality (entity extraction)
- **TEST 5**: Performance check (response time)
- **TEST 6**: Multiple concurrent sessions (isolation)

### 2. `/scripts/test-conversation-compression.sh`
Manual bash script for quick compression testing via curl commands.

## Test Execution

### Option 1: Playwright E2E Tests (Recommended for CI/CD)

```bash
# Start development server
./scripts/dev-with-keys.sh

# In another terminal, run tests
npx playwright test conversation-memory --headed

# For multi-tenant testing, override tenant_id:
TENANT_ID=my-hotel npx playwright test conversation-memory --headed
```

**Note**: Tests require manual verification via SQL queries output in console.

### Option 2: Manual Testing Script (Faster for Development)

```bash
# Start development server
./scripts/dev-with-keys.sh

# Run manual test (default: simmerdown)
./scripts/test-conversation-compression.sh

# Test with different tenant
./scripts/test-conversation-compression.sh my-hotel

# Or use environment variable
TENANT_ID=custom-tenant ./scripts/test-conversation-compression.sh
```

This script:
- Creates test sessions for specified tenant
- Sends 18, 20, and 30 messages
- Outputs SQL queries for manual verification
- Provides cleanup commands
- **Multi-tenant support**: Default 'simmerdown', override via CLI arg or env var

## Verification Results (2025-10-03)

### Manual Verification via MCP Supabase Tool

```sql
SELECT
  session_id,
  message_range,
  message_count,
  summary_text,
  key_entities
FROM conversation_memory
ORDER BY created_at DESC
LIMIT 2;
```

**Results:**
| session_id | message_range | message_count | summary_length | status |
|------------|---------------|---------------|----------------|---------|
| bb3731d4-963e-4943-bdb9-9e7825784a43 | messages 1-10 | 10 | 71 chars | âœ… Compressed |
| 27ddd549-09bd-4b82-b960-b49ff730e044 | messages 1-10 | 10 | 71 chars | âœ… Compressed |

**Conversation History Check:**
```sql
SELECT
  session_id,
  jsonb_array_length(conversation_history) as msg_count
FROM prospective_sessions
WHERE session_id = 'bb3731d4-963e-4943-bdb9-9e7825784a43';
```

**Result:**
- âœ… Conversation history trimmed to **10 messages** (messages 6-10 kept)
- âœ… First 10 messages compressed and removed from active history
- âœ… message_range correctly set to "messages 1-10"

## Test Coverage

### âœ… PASSING Tests
1. **Compression Trigger at 20 Messages**: Confirmed working
2. **History Trimming**: First 10 messages removed, last 10 kept
3. **Database Persistence**: conversation_memory table correctly populated
4. **Message Range Tracking**: Correct range "messages 1-10"
5. **Session Isolation**: Each session compresses independently

### âš ï¸ ISSUES FOUND
1. **Compression Summary Error**:
   - Summary text shows: "Error al comprimir conversacion (10 mensajes). Contenido no disponible."
   - This indicates Claude API call failed during compression
   - **Root Cause**: Likely API key issue or rate limiting
   - **Impact**: Medium - compression still saves space, but summary quality degraded
   - **Fix Needed**: Investigate anthropic client initialization in conversation-compressor.ts

2. **Entity Extraction Fallback**:
   - key_entities shows fallback: `{"key_questions":[],"travel_intent":{"preferences":[]},"topics_discussed":["error_compression"]}`
   - This is expected when compression fails, but should be rare

## Test Quality Metrics

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| Compression Trigger | At 20 messages | At 20 messages | âœ… PASS |
| History Trim | 10 messages kept | 10 messages kept | âœ… PASS |
| Database Insert | 1 row | 1 row | âœ… PASS |
| Message Range | "messages 1-10" | "messages 1-10" | âœ… PASS |
| Summary Quality | >50 chars | 71 chars (error msg) | âš ï¸ DEGRADED |
| Embedding Dims | 1024 | Not verified | ğŸ”„ PENDING |
| Response Time | <5000ms | Not measured | ğŸ”„ PENDING |

## Recommendations

### Immediate Actions
1. **Fix Compression Summary Error**:
   ```bash
   # Check ANTHROPIC_API_KEY is set correctly
   echo $ANTHROPIC_API_KEY

   # Verify API key in .env or dev script
   grep ANTHROPIC_API_KEY .env.local
   ```

2. **Verify Embedding Generation**:
   ```sql
   SELECT
     session_id,
     embedding_fast IS NOT NULL as has_embedding
   FROM conversation_memory
   WHERE session_id = 'bb3731d4-963e-4943-bdb9-9e7825784a43';
   ```

3. **Add Performance Monitoring**:
   - Log compression duration in conversation-compressor.ts
   - Track Claude API response times
   - Monitor OpenAI embedding generation latency

### Future Improvements
1. **Automated SQL Verification**: Create test helper API endpoint for database queries
2. **Embedding Quality Tests**: Verify embeddings are not dummy fallback vectors
3. **Load Testing**: Test with 100+ concurrent compressions
4. **Error Recovery**: Test compression retry logic when APIs fail

## How to Run Tests in Production

### Pre-Deployment Checklist
- [ ] ANTHROPIC_API_KEY configured
- [ ] OPENAI_API_KEY configured
- [ ] Supabase connection healthy
- [ ] conversation_memory table exists
- [ ] prospective_sessions table has conversation_history column

### Execution
```bash
# 1. Start server
./scripts/dev-with-keys.sh

# 2. Run quick manual test (3 minutes)
./scripts/test-conversation-compression.sh

# 3. Verify results in Supabase SQL Editor
# Copy SQL queries from script output

# 4. Cleanup test data
# Run DELETE commands from script output
```

### CI/CD Integration (Future)
```yaml
# .github/workflows/test-conversation-memory.yml
name: Conversation Memory Tests

on: [push]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - run: npm install
      - run: npx playwright install
      - run: ./scripts/dev-with-keys.sh &
      - run: sleep 15 # Wait for server
      - run: npx playwright test conversation-memory
      - run: ./scripts/cleanup-test-sessions.sh
```

## Cleanup

After testing, always cleanup test data:

```sql
-- Find test sessions
SELECT session_id, created_at
FROM prospective_sessions
WHERE tenant_id = '5fa5ec02-f77f-45e4-a9bd-0f9e5c5e5e5e'
ORDER BY created_at DESC
LIMIT 20;

-- Delete conversation memories
DELETE FROM conversation_memory
WHERE session_id IN ('bb3731d4-963e-4943-bdb9-9e7825784a43', 'a16875e9-fb75-4bf2-970a-8011243b6ccd');

-- Delete sessions
DELETE FROM prospective_sessions
WHERE session_id IN ('bb3731d4-963e-4943-bdb9-9e7825784a43', 'a16875e9-fb75-4bf2-970a-8011243b6ccd');
```

## Conclusion

**Auto-compression system is FUNCTIONAL** with the following status:
- âœ… Core logic working (trigger, trim, persist)
- âš ï¸ Summary generation degraded (API error)
- ğŸ”„ Embedding quality not yet verified
- ğŸ”„ Performance metrics not yet captured

**Next Steps**:
1. Fix Anthropic API client initialization issue
2. Verify embedding generation works correctly
3. Add performance logging
4. Run full 30-message test to verify second compression

---

**Test Date**: 2025-10-03
**Tested By**: Claude Code (backend-developer agent)
**Test Environment**: Local development (localhost:3000)
**Database**: Supabase (production instance)
